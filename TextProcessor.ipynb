{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TextProcessor\n",
    "\n",
    "### Set environmental variables\n",
    "\n",
    "In order to properly load modules within this notebook from outside the repository folder, set the script **PATH** below,  e.g. ```C:/TextProcessor```:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH = \"/path/to/TextProcessor\" # <-- optional if running from native path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import importlib.util, os\n",
    "\n",
    "if not os.path.isdir(PATH):\n",
    "    PATH = os.getcwd()\n",
    "PATH = os.path.realpath(PATH)\n",
    "\n",
    "spec = importlib.util.spec_from_file_location(\"__init__\", PATH+'/__init__.py')\n",
    "init = importlib.util.module_from_spec(spec)\n",
    "spec.loader.exec_module(init)\n",
    "\n",
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from TextProcessor import TextProcessor\n",
    "from kwic import run_kwic\n",
    "from nltklib import ngram_an\n",
    "from spacylib import nlp_an\n",
    "from textractlib import textract_file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Process text from document\n",
    "\n",
    "Analyzes keywords in context, n-grams, part-of-speech tags, noun phrases, verbs and entities, writing all output to CSV files.\n",
    "\n",
    "**Hint:** the `keywords` parameter accepts either a single word, a list, a string list (comma separated) or an integer (for highest occurrences)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"\" # <-- string or document file with content to parse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TextProcessor(text,\n",
    "              output_folder='RESULTS',\n",
    "              keywords='',\n",
    "              kwic_size=5,\n",
    "              ngram_size=2,\n",
    "              ignore_case=False,\n",
    "              column_csv='',\n",
    "              allow_download=True,\n",
    "              language='auto',\n",
    "              encode='utf-8',\n",
    "              convert=False,\n",
    "              binary='')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Advanced usage\n",
    "\n",
    "#### Read/convert document file\n",
    "\n",
    "Supported formats include `.doc(x)`, `.epub`, `.odt` and `.pdf`. **To-do:** add whole folder conversion with recursion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = textract_file(text, convert=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### KWIC Analysis\n",
    "\n",
    "Look for keywords in context in a text file or string, return associated tags. **Hint:** try changing associated `size` (5) for different results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keywords = \"\" # <-- keywords to analyze context (optional)\n",
    "\n",
    "run_kwic(text, keywords, size=5, ignore_case=False, display=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### N-gram analysis\n",
    "\n",
    "Requires the Natural Language Toolkit installed. **Hint:** try changing the default `n-value` (2) for different results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "grams = ngram_an(text, n=2)\n",
    "for g in grams:\n",
    "    print(g)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Syntax analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perform analysis using `spaCy`, a library for Natural Language Processing featuring NER, POS tagging, dependency parsing, word vectors and more.\n",
    "\n",
    "**Note:** it is required to download the [NLP model](https://spacy.io/usage/models) if you're running for the first time e.g. by executing ```python -m spacy download en``` for english."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_tags, noun_chunks, verbs, entities = nlp_an(text, language='auto', allow_download=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!python -m spacy download en"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Inspect output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('POS tags:')\n",
    "for pos in pos_tags:\n",
    "    print(pos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Noun phrases:')\n",
    "for chunk in noun_chunks:\n",
    "    print(chunk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Verbs:')\n",
    "for verb in verbs:\n",
    "    print(verb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Entities:')\n",
    "for ent in entities:\n",
    "    print(ent)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Compress output â†’  `output.zip`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!zip output.zip *csv *txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [Download output files](output.zip)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_____\n",
    "\n",
    "### References\n",
    "\n",
    "* Natural Language Toolkit: https://www.nltk.org/\n",
    "\n",
    "* spaCy documentation: https://spacy.io\n",
    "\n",
    "* textract documentation: http://textract.readthedocs.io/en/latest/\n",
    "\n",
    "* textract @ GitHub: https://github.com/deanmalmgren/textract\n",
    "\n",
    "* tika-python @ GitHub: https://github.com/chrismattmann/tika-python\n",
    "\n",
    "* KWIC implementation by @vahbuna: https://github.com/vahbuna/kwic/"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
