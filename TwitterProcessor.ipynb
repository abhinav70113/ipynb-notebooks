{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TwitterProcessor\n",
    "\n",
    "### Set environmental variables\n",
    "\n",
    "In order to properly load modules within this notebook from outside the repository folder, set the script **PATH** below,  e.g. ```C:/TwitterProcessor```:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH = \"/media/data/scripts/chn@git/chn-tools/tools/TwitterProcessor\" # <-- optional if running from native path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib.util, os\n",
    "\n",
    "if not os.path.isdir(PATH):\n",
    "    PATH = os.getcwd()\n",
    "PATH = os.path.realpath(PATH)\n",
    "\n",
    "spec = importlib.util.spec_from_file_location(\"__init__\", PATH+'/__init__.py')\n",
    "init = importlib.util.module_from_spec(spec)\n",
    "spec.loader.exec_module(init)\n",
    "\n",
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.offline as py\n",
    "\n",
    "from tools import TwitterProcessor\n",
    "\n",
    "from tools.DataFrames.dflib import *\n",
    "from tools.DataFrames.filter import *\n",
    "from tools.TwitterProcessor.TweetParser import *\n",
    "from tools.TwitterProcessor.worldmap import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "py.init_notebook_mode(connected=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load tweets data frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tweets = tweets_load(file_name) # <-- ensure older format compability"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Select specific interval to filter data frame (optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_date = \"1970-01-01\"\n",
    "max_date = \"2038-01-18 03:14:07\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "tweets = df_filter_timestamp(tweets, min_date, max_date, column=\"timestamp\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Select only tweets that match a text filter rule (optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"trump|Trump\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "tweets = df_filter_text(tweets, text, column=\"tweet_text\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parse tweets and generate output data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_parse(tweets, stop_words='english', output='RESULTS')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Choropleth world map\n",
    "\n",
    "Accepted format for `country_code` is 3-letters long by default. **Tip:** uncomment `line 7` below to enable writing to `wordmap.html`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "locations = 'RESULTS/locations.csv'\n",
    "\n",
    "df = df_worldmap(locations) # leave blank to check\n",
    "\n",
    "plot_worldmap(df,\n",
    "              name='Worldmap',\n",
    "              #output='worldmap.html',\n",
    "              inline=True,\n",
    "              auto_open=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Compress output â†’  `output.zip`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!zip output.zip/*{csv,xls,xlsx,png,html}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [Download output files](output.zip)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Calculate metrics indices `JS` `LEGACY`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sqlnode = abspath(PATH+'/tools/TwitterClusterJs')\n",
    "tweets  = sqlnode_tweets(tweets, sqlnode=sqlnode)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Start TwitterProcessor `LEGACY`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ta, tg, ml, tp = TwitterProcessor.__init__()\n",
    "# ta = TweetAnalytics()\n",
    "# tg = TweetGraph()\n",
    "# ml = TweetML()\n",
    "# tp = TweetProcessor()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generate preformatted analytics report `LEGACY`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent_model = abspath(PATH+'/tools/TwitterProcessor/twitter_sentiment_model.h5')\n",
    "word2vec = abspath(PATH+'/tools/TwitterProcessor/word2vec_twitter_model.bin')\n",
    "\n",
    "if os.path.isfile(sent_model): \n",
    "    # load pretrained sentiment model\n",
    "    ml.load_sentiment_model(sent_model)\n",
    "\n",
    "if os.path.isfile(word2vec):\n",
    "    # load pretrained word embedding\n",
    "    tp.load_word_embedding_from_file(word2vec)\n",
    "else: # train new word embedding\n",
    "    tp.trainWordEmbedding(tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create new topic model\n",
    "ml.create_topic_model_LDA(tweets,\n",
    "                          tp, # TweetProcessor\n",
    "                          num_topics=5,\n",
    "                          extra_stopwords=[])\n",
    "\n",
    "# append topic to tweet time slice\n",
    "tp.add_topic(tweets, ml)\n",
    "\n",
    "# append sentiment to tweet time slice\n",
    "tp.add_sentiment(tweets, ml)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write report to file\n",
    "ta.write_report(tweets,\n",
    "                ml, # TweetML\n",
    "                'report.xls',\n",
    "                num_lines_per_topic=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tweet topic modeling `LEGACY`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = 'insert_text_here'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get topics\n",
    "ml.get_LDA_topics()\n",
    "\n",
    "# predict topics\n",
    "ml.predict_LDA_topic(text, tp)\n",
    "\n",
    "# find clusters\n",
    "topic_model = ml.findTopicClusters(tp.word_embedding_model, n_clusters=5)\n",
    "\n",
    "# sort topic model\n",
    "sorted_topic_model = ml.sortTopicModel(topic_model)\n",
    "\n",
    "# print to examine\n",
    "ml.printTopicModel(sorted_topic_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Write to output `LEGACY`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define topic cluster\n",
    "topic_number = 0\n",
    "\n",
    "# find tweets from a specific cluster\n",
    "topic_specific_tweet_list = ml.findTweetsTopicSpecific(topic_model[0][topic_number], tweet_data)\n",
    "\n",
    "# write tweets of a specific topic to CSV\n",
    "ml.printTweetsTopicSpecificToCSV(topic_specific_tweet_list, file_path='tweet_specific_topics.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___________________________________"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ta = TweetAnalytics()\n",
    "# ta.all_tweet_text(tweet_data, topic_number=0)\n",
    "# ta.sentiment_per_topic(tweet_data)\n",
    "# ta.top_by_group_by_topic(tweet_data, by='retweets_plus_favorites', topic_number=0, num_influencers=5)\n",
    "# ta.top_media_by_topic(tweet_data, topic_number=0)\n",
    "# ta.top_topics_by_count(tweet_data)\n",
    "# ta.unique_tweet_text(tweet_data, topic_number=0)\n",
    "# ta.write_report(tweet_data, ml, filename='./ta_report.xls', num_lines_per_topic=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ml = TweetML()\n",
    "# ml.create_topic_model_LDA(tweet_data, tweet_preprocessor, num_topics=5, extra_stopwords=[])\n",
    "# ml.findOptimalNumberClusters(model, num_clusters_to_try)\n",
    "# ml.findTopicClusters(model, n_clusters)\n",
    "# ml.findTweetsTopicSpecific(topic_list, tweet_data, sort=True)\n",
    "# ml.flatten_2D_trainingset(X)\n",
    "# ml.get_LDA_topics()\n",
    "# ml.groupSentenceIntoTopicClusters(sentence, word_clusters_dict, tp)\n",
    "# ml.load_sentiment_model(filepath='twitter_sentiment_model.h5')\n",
    "# ml.neuralNetModel_Conv_Flattened()\n",
    "# ml.neuralNetModel_Conv_Sequential_1D()\n",
    "# ml.plotTopicClusters(word2vec_model)\n",
    "# ml.predict_LDA_topic(tweet_text, tweet_preprocessor)\n",
    "# ml.predict_sentiment(tweet_text, tweet_preprocessor)\n",
    "# ml.printTopicClusterSize(word_clusters_list)\n",
    "# ml.printTopicModel(topic_model)\n",
    "# ml.printTweetsTopicSpecificToCSV(tweet_list_topic_specific, file_path='topic_specific_tweet_list.csv')\n",
    "# ml.sentiment_model_nn_conv2d_seq(X_train, Y_train, batch_size=1000, epochs=10, )\n",
    "# ml.sortTopicModel(topic_model)\n",
    "# ml.tweetsToCSV(tweet_list_topic_specific)\n",
    "# ml.create_test_set(test_tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tg = TweetGraph()\n",
    "# tg.main()\n",
    "# tg.prepare_graph_with_attributes(tweet_data, add_sentiment=True, add_topic=True)\n",
    "# tg.prepare_graph_without_attributes(tweet_data)\n",
    "# tg.build_at_graph()\n",
    "# tg.build_complete_graph()\n",
    "# tg.build_rt_graph()\n",
    "# tg.clear_all()\n",
    "# tg.export_graph_gsv(gexf_file='postgrowth_atgraph.gexf')\n",
    "# tg.load_data_for_graph(search_string='#blackpanther', start_date=None, end_date=None)\n",
    "# tg.show_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tp = TweetProcessor()\n",
    "# tp.add_sentiment(tweet_data, ml_sentiment)\n",
    "# tp.add_topic(tweet_data, ml_topic_model)\n",
    "# tp.addViralityIndex(tweet_data)\n",
    "# tp.avg_tweet_length(tweet_data)\n",
    "# tp.connect_DB()\n",
    "# tp.create_prediction_set_sentiment(tweet_data)\n",
    "# tp.create_prediction_set_sentiment_from_list(X_input=[])\n",
    "# tp.create_sentence_embedding_from_list(sentence_list)\n",
    "# tp.create_training_set_sentiment(tweet_data)\n",
    "# tp.createTrainingSetLDATopicModel(tweet_data, extra_stopwords=[])\n",
    "# tp.createTrainingSetTweetVirality(tweet_data, X, Y)\n",
    "# tp.createTrainingSetWordEmbedding(tweet_data, X)\n",
    "# tp.drop_duplicate_tweets(tweet_data)\n",
    "# tp.filter_tweets_by_date(tweet_data)\n",
    "# tp.find_duplicate_tweets(tweet_data)\n",
    "# tp.getWordListAndVectors(word2vec_model)\n",
    "# tp.is_number(s)\n",
    "# tp.load_all_graph_tweet_data_by_search_mysql(search_string, start_date=None, end_date=None)\n",
    "# tp.load_all_graph_tweet_data_by_search_mysql_eliminate_duplicate_retweets(search_string, start_date=None, end_date=None)\n",
    "# tp.load_all_report_tweet_data_by_search_mysql(search_string, start_date=None, end_date=None)\n",
    "# tp.load_all_retweet_data_mysql()\n",
    "# tp.load_all_tweet_data_by_search_mysql(search_string, start_date=None, end_date=None)\n",
    "# tp.load_all_tweet_data_mysql(search_string, start_date=None, end_date=None)\n",
    "# tp.load_random_tweet_data()\n",
    "# tp.load_tweet_data_by_topic_and_id(search_string, start_id=None, end_id=None, drop_duplicates=True)\n",
    "# tp.load_tweets(table_name=None, org_id=0, narr_id=1, scrape_id=None)\n",
    "# tp.load_tweets_csv(filepath='Sentiment Analysis Dataset.csv', num_rows=MAX_TWEETS_TO_LOAD)\n",
    "# tp.load_tweets_mysql(search_string)\n",
    "# tp.load_word_embedding_from_file(filepath='word2vec_twitter_model.bin')\n",
    "# tp.load_wordembedding_twitter_search()\n",
    "# tp.loadStopwords(stopwords_list=[])\n",
    "# tp.sentence2embedding(sentence, extra_stopwords=[], dimensions=EMBEDDING_DIMENSIONS)\n",
    "# tp.sentence2tokens(sentence)\n",
    "# tp.trainWeightedWordEmbedding(tweet_data, max_word_dimensions=100, max_vocab_size=None, window=5, extra_stopwords=[], min_count=5, epochs=10)\n",
    "# tp.trainWordEmbedding(tweet_data, max_word_dimensions=EMBEDDING_DIMENSIONS, max_vocab_size=None, window=5, extra_stopwords=[], min_count=5, epochs=10)\n",
    "# tp.trainWordEmbedding_iloc(tweet_data, max_word_dimensions=EMBEDDING_DIMENSIONS, max_vocab_size=None, window=5, extra_stopwords=[], min_count=5, epochs=10)\n",
    "# tp.word2vec(word)\n",
    "# tp.write_to_parsing_script_report(tweet_data, filepath='parsing_script_report.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "\n",
    "### References\n",
    "\n",
    "* Cloropleth Maps @ plot.ly: https://plot.ly/python/choropleth-maps/"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
